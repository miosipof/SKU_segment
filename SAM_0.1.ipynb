{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc36a8d-9a02-4f88-ad53-d28cd6b86b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m ipykernel install --user --name=grounded_sam2_env\n",
    "!source grounded_sam2_env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72e803e-f6a3-4755-94fa-601e7a4091b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import Image, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c80c2e2-a98f-4190-8b8e-32044c6bf90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 2.6.0+cu124 (NVIDIA GeForce RTX 3090)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Setup complete. Using torch {torch.__version__} \" \\\n",
    "      f\"({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bac632-2a27-499a-b4b6-6a3ceadfa8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = \"/SKU110K_fixed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a6b3a-87a5-4e99-b93f-4782f14e70e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c1793d-de56-47d5-b977-f501efdf3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations = pd.read_csv(\"/SKU110K_fixed/annotations/annotations_test.csv\")\n",
    "# annotations.columns=['image_name','x1','y1','x2','y2','class','image_width','image_height']\n",
    "# annotations.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13036b19-0355-4890-b178-25476bf7d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data = {\n",
    "#     'train': None,\n",
    "#     'val': None,\n",
    "#     'test': None\n",
    "# }\n",
    "\n",
    "# for split in raw_data.keys():\n",
    "#     annotations = pd.read_csv(f\"/SKU110K_fixed/annotations/annotations_{split}.csv\")\n",
    "#     annotations.columns=['image_name','x1','y1','x2','y2','class','image_width','image_height']\n",
    "#     raw_data[split] = annotations\n",
    "#     print(f\"{split} split: {len(annotations)} annotations, {len(annotations.groupby('image_name'))} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de67fe4-c48d-4d35-aa30-f91c3f540cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = {}\n",
    "\n",
    "# for split, data in raw_data.items():\n",
    "    \n",
    "#     data = data.reset_index()\n",
    "#     images = set(data['image_name'])\n",
    "\n",
    "#     ds[split] = []\n",
    "\n",
    "#     for i, image_name in enumerate(list(images)):\n",
    "#         df = data[data['image_name'] == image_name]\n",
    "\n",
    "#         img_path = os.path.join(ds_path,image_name)\n",
    "#         bboxes = []\n",
    "\n",
    "#         for idx, ann in df.iterrows():\n",
    "#             bbox = [ann['x1'],ann['x2'],ann['y1'],ann['y2']]\n",
    "#             bboxes.append(bbox)        \n",
    "\n",
    "#         ds[split].append({\"image_path\": img_path, \"bboxes\": bboxes})\n",
    "#         if i%50 == 0:\n",
    "#             print(f\"{100*i/len(images):.1f}% of {split} split processed\")\n",
    "\n",
    "\n",
    "# with open('sku110_dataset.json', 'w') as f: \n",
    "#     json.dump(ds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda16ce2-a769-41bc-a57a-8e86537de2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8219\n"
     ]
    }
   ],
   "source": [
    "with open('sku110_dataset.json') as f:\n",
    "    ds = json.load(f)\n",
    "\n",
    "print(len(ds['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d80f3e1-eb58-4a8c-8afc-05d057ee5aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install supervision\n",
    "# !pip install iopath\n",
    "# !pip install addict\n",
    "# !pip install yapf\n",
    "# !pip install pycocotools\n",
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2248aaac-c0f7-48e9-b5e4-3547072a8fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n"
     ]
    }
   ],
   "source": [
    "# Grounding DINO SAM-2\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from sam2.build_sam import build_sam2_video_predictor, build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection \n",
    "from utils.track_utils import sample_points_from_masks\n",
    "from utils.video_utils import create_video_from_images\n",
    "from grounding_dino.groundingdino.util.inference import load_model, predict, load_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c287a87-7890-4e29-b58e-e769419257d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc2bf22e-8d3b-47de-8dc8-76b648d10c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video predictor loaded from config configs/sam2.1/sam2.1_hiera_l.yaml, checkpoint ./checkpoints/sam2.1_hiera_large.pt\n",
      "SAM2 image model loaded from ./checkpoints/sam2.1_hiera_large.pt\n",
      "image_predictor loaded from SAM2 image model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Grounding model loaded from config grounding_dino/groundingdino/config/GroundingDINO_SwinT_OGC.py, checkpoint checkpoints/groundingdino_swint_ogc.pth\n",
      "Model converted to float32\n"
     ]
    }
   ],
   "source": [
    "# use bfloat16 for the entire notebook\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# init sam image predictor and video predictor model\n",
    "sam2_checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "video_predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint)\n",
    "print(f\"Video predictor loaded from config {model_cfg}, checkpoint {sam2_checkpoint}\")\n",
    "sam2_image_model = build_sam2(model_cfg, sam2_checkpoint)\n",
    "print(f\"SAM2 image model loaded from {sam2_checkpoint}\")\n",
    "image_predictor = SAM2ImagePredictor(sam2_image_model)\n",
    "print(f\"image_predictor loaded from SAM2 image model\")\n",
    "\n",
    "grounding_model_config = \"grounding_dino/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "grounding_model_weights = \"checkpoints/groundingdino_swint_ogc.pth\"\n",
    "grounding_model = load_model(grounding_model_config, grounding_model_weights)\n",
    "print(f\"Grounding model loaded from config {grounding_model_config}, checkpoint {grounding_model_weights}\")\n",
    "grounding_model = grounding_model.float()\n",
    "print(f\"Model converted to float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77eae1-d19c-4405-a55e-82fc077aea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded from /SKU110K_fixed/images/train_7763.jpg\n",
      "Using prompt: product . object\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 2: Prompt Grounding DINO and SAM image predictor to get the box and mask for specific frame\n",
    "\"\"\"\n",
    "\n",
    "# Load image\n",
    "image_path = os.path.join(ds_path, 'images', 'train_7763.jpg')\n",
    "image_source, image = load_image(image_path)  # PIL and Tensor image\n",
    "print(f\"Image loaded from {image_path}\")\n",
    "\n",
    "# Define text prompt\n",
    "prompt = \"product . object\"  # multiple objects separated by ' . '\n",
    "print(f\"Using prompt: {prompt}\")\n",
    "\n",
    "# Run prediction (includes processing internally)\n",
    "with torch.cuda.amp.autocast(enabled=False): # \"ms_deform_attn_forward_cuda\" not implemented for 'BFloat16'\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=grounding_model,\n",
    "        image=image,\n",
    "        caption=prompt,\n",
    "        box_threshold=0.3,\n",
    "        text_threshold=0.25\n",
    "    )\n",
    "\n",
    "print(boxes, logits, phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f54e9-6328-45a2-8816-fc462b2e65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes_on_image(image_path, boxes, phrases, box_color='red', text_color='white'):\n",
    "    \"\"\"\n",
    "    Plots bounding boxes and phrases on an image.\n",
    "    :param image_path: Path to the image file.\n",
    "    :param boxes: torch.Tensor of shape (N, 4), normalized [x1, y1, x2, y2]\n",
    "    :param phrases: List of strings, same length as boxes.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    width, height = image.size\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, phrase in zip(boxes, phrases):\n",
    "        x1, y1, x2, y2 = box\n",
    "        x1 *= width\n",
    "        x2 *= width\n",
    "        y1 *= height\n",
    "        y2 *= height\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "\n",
    "        rect = patches.Rectangle((x1, y1), w, h, linewidth=2, edgecolor=box_color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1 - 5, phrase, color=text_color, fontsize=12, bbox=dict(facecolor=box_color, alpha=0.5))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_boxes_on_image(image_path, boxes, phrases)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grounded_sam2_env",
   "language": "python",
   "name": "grounded_sam2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
